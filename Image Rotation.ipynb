{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Idea:\n",
    "# /content/gdrive/My Drive/Colab Notebooks/\n",
    "\n",
    "# Plan\n",
    "# 1. Add Background to Image dependent on image size (choose from x different backgrounds)\n",
    "# 2. Add some more distortions\n",
    "# 3. Rotate and sheer image\n",
    "# 4. Rescale images to fixed size.\n",
    "# 5. Write as generator\n",
    "# 6. Write CNN which predicts 2 angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_foreground_size = 300\n",
    "scaler = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_background_img(foreground, background, max_foreground_size, scaler):\n",
    "    width = foreground.size[0]\n",
    "    height = foreground.size[1]\n",
    "\n",
    "    if(width > height):\n",
    "        percentage = max_foreground_size/width\n",
    "        max_size = int(width*scaler*percentage)\n",
    "    else:\n",
    "        percentage = max_foreground_size/height\n",
    "        max_size = int(height*scaler*percentage)\n",
    "        \n",
    "#     print(width, height, percentage)\n",
    "\n",
    "    foreground = foreground.resize((int(width*percentage), int(height*percentage)), Image.ANTIALIAS)\n",
    "\n",
    "    background = background.resize((max_size, max_size), Image.ANTIALIAS)\n",
    "\n",
    "    margin_w = int((background.size[0]-foreground.size[0])/2)\n",
    "    margin_h = int((background.size[1]-foreground.size[1])/2)\n",
    "\n",
    "    # foreground.show()\n",
    "    background.paste(foreground, (margin_w, margin_h))\n",
    "#     background.show()\n",
    "\n",
    "    return background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(foreground, background, max_foreground_size, scaler):\n",
    "    # load image with background\n",
    "    open_cv_image = np.array(add_background_img(foreground, background, max_foreground_size, scaler).convert('RGB'))\n",
    "\n",
    "    # Convert RGB to BGR\n",
    "    open_cv_image = open_cv_image[:, :, ::-1].copy() \n",
    "\n",
    "    # define augmentations\n",
    "    rotation = random.uniform(-85,85)\n",
    "    shear = random.uniform(-10,10)\n",
    "\n",
    "    # 2. Add some more distortions\n",
    "    blur_aug = ia.augmenters.blur.MotionBlur(k=(3,10), angle=(0, 360), direction=(-1.0, 1.0))\n",
    "    \n",
    "    # 3. Rotate and sheer image\n",
    "    rotate_aug = ia.augmenters.geometric.Affine(rotate=rotation)\n",
    "    shear_aug = ia.augmenters.geometric.Affine(shear=shear)\n",
    "    \n",
    "    # exectue augmentation\n",
    "    new_img = blur_aug.augment_image(open_cv_image)\n",
    "    new_img = rotate_aug.augment_image(new_img)\n",
    "    new_img = shear_aug.augment_image(new_img)\n",
    "\n",
    "    # show\n",
    "    img = cv2.cvtColor(new_img, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "#     img.show()\n",
    "\n",
    "    # crop image\n",
    "    rand_scaler = scaler*random.uniform(0.7,1.3)\n",
    "#     print(\"rand_scaler\", rand_scaler)\n",
    "    crop_side_percentage = (rand_scaler-1)/(2*rand_scaler)\n",
    "\n",
    "    area = (\n",
    "        img.size[0]*crop_side_percentage, # width left\n",
    "        img.size[1]*crop_side_percentage, # height top\n",
    "        img.size[0]*(1-crop_side_percentage), # width right\n",
    "        img.size[1]*(1-crop_side_percentage), # height bottom\n",
    "    )\n",
    "    cropped_img = img.crop(area)\n",
    "#     cropped_img.show()\n",
    "    \n",
    "    return cropped_img, rotation, shear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test augmentation\n",
    "\n",
    "# background = Image.open(\"background_img/Table_surface.jpg\")\n",
    "# foreground = Image.open(\"invoice_img_data/out_0.png\")\n",
    "\n",
    "# final_img, rotation, shear = augment_image(foreground, background, max_foreground_size, scaler)\n",
    "# final_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_folder = \"invoice_img_data\"\n",
    "background_img_folder = \"background_img\"\n",
    "\n",
    "input_img_names = [x for x in os.listdir(input_data_folder) if \"_label\" not in x and \".png\" in x]\n",
    "background_img_names = [\n",
    "    x for x in os.listdir(background_img_folder) if \"_label\" not in x and \".png\" in x or \".jpg\" in x or \".jpeg\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 23)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_img_names), len(background_img_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# https://stackoverflow.com/questions/42480111/model-summary-in-pytorch\n",
    "from torchsummary import summary\n",
    "\n",
    "# https://github.com/lanpa/tensorboardX\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(\"logs/image_rotation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class depthwise_separable_conv(nn.Module):\n",
    "    def __init__(self, nin, nout, ksize, padd):\n",
    "        super(depthwise_separable_conv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=ksize, padding=padd, groups=nin)\n",
    "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_max_step(nn.Module):\n",
    "    def __init__(self, nin, nout, ksize, padd):\n",
    "        super(conv_max_step, self).__init__()\n",
    "        self.conv = depthwise_separable_conv(nin, nout, ksize, padd)\n",
    "        self.batchn = nn.BatchNorm2d(nout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxp = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, max_img_size, nchannel, nclasses, ):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        nin = nchannel\n",
    "        nout = int(nin*2)\n",
    "        self.cm1 = conv_max_step(nin, nout, 7, 3)\n",
    "        max_img_size = max_img_size/2\n",
    "\n",
    "        nin = nout\n",
    "        nout = int(nin*2)\n",
    "        self.cm2 = conv_max_step(nin, nout, 3, 1)\n",
    "        max_img_size = max_img_size/2\n",
    "\n",
    "        nin = nout\n",
    "        nout = int(nin*2)\n",
    "        self.cm3 = conv_max_step(nin, nout, 3, 1)\n",
    "        max_img_size = max_img_size/2\n",
    "\n",
    "        nin = nout\n",
    "        nout = int(nin*2)\n",
    "        self.cm4 = conv_max_step(nin, nout, 3, 1)\n",
    "        max_img_size = int(max_img_size/2)\n",
    "\n",
    "        self.lin_dim = nout*max_img_size*max_img_size\n",
    "        self.fc = nn.Linear(in_features=self.lin_dim, out_features=nclasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cm1(x)\n",
    "        x = self.cm2(x)\n",
    "        x = self.cm3(x)\n",
    "        x = self.cm4(x)\n",
    "        x = x.view(-1, self.lin_dim)\n",
    "        out = self.fc(x)\n",
    "       \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data_generator(input_img_names, background_img_names, itr, batch_size):\n",
    "    inpu = []\n",
    "    rotation = []\n",
    "    shear = []\n",
    "\n",
    "    # put into generator function for evaluation\n",
    "    for img_name in input_img_names[itr*batch_size:(itr+1)*batch_size]:\n",
    "        background_name = random.choice(background_img_names)\n",
    "\n",
    "        foreground = Image.open(os.path.join(input_data_folder, img_name))\n",
    "        background = Image.open(os.path.join(background_img_folder, background_name))\n",
    "\n",
    "        curr_img, rot, she = augment_image(foreground, background, max_foreground_size, scaler)\n",
    "\n",
    "        curr_img = curr_img.resize((max_img_size, max_img_size), Image.ANTIALIAS)\n",
    "        curr_img = np.array(curr_img)\n",
    "\n",
    "        inpu.append(curr_img)\n",
    "        rotation.append(rot)\n",
    "        shear.append(she)\n",
    "\n",
    "    # move channel to second index position\n",
    "    inpu = np.swapaxes(np.array(inpu), 3, -3)\n",
    "    X = torch.from_numpy(inpu).float()\n",
    "\n",
    "    y = np.transpose(np.vstack([rotation, shear]))\n",
    "    y = torch.from_numpy(y).float()\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "17060.29296875\n",
      "epoch: 1\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 50\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "\n",
    "n_batches = int(np.ceil(len(input_img_names)/batch_size))\n",
    "\n",
    "max_img_size = 64\n",
    "inp_channels = 3\n",
    "nclasses = 2\n",
    "\n",
    "# model\n",
    "model = Model(max_img_size, inp_channels, nclasses)\n",
    "# loss\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"epoch: %i\" % epoch)\n",
    "    for itr in range(n_batches):\n",
    "        # get training data from generator\n",
    "        X, y = training_data_generator(input_img_names, background_img_names, itr, batch_size)\n",
    "\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = model.forward(X)\n",
    "\n",
    "        # Compute and print loss.\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        # Zero the gradients before running the backward pass.\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss.\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    print(loss.item())\n",
    "    writer.add_scalar(\"total_loss\", loss.item(), epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sudo Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_image(final_img, shear, rotation):\n",
    "    shear_aug_ = ia.augmenters.geometric.Affine(shear=-shear)\n",
    "    rotate_aug_ = ia.augmenters.geometric.Affine(rotate=-rotation)\n",
    "\n",
    "    reverse_img = np.array(final_img.convert('RGB'))\n",
    "\n",
    "    # Convert RGB to BGR\n",
    "    reverse_img = reverse_img[:, :, ::-1].copy()\n",
    "\n",
    "    # reverse augmentation\n",
    "    reverse_img = shear_aug_.augment_image(reverse_img)\n",
    "    reverse_img = rotate_aug_.augment_image(reverse_img)\n",
    "\n",
    "    # show\n",
    "    img = cv2.cvtColor(reverse_img, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_image(final_img).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0mtypekey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'typestr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2358\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2359\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 64), '|u1')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3b328758e501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcorrect_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshear\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2359\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;31m# print(typekey)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2361\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot handle this data type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2362\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type"
     ]
    }
   ],
   "source": [
    "for i in range(batch_size):\n",
    "    Image.fromarray(inpu[i]).show()\n",
    "    correct_image(Image.fromarray(inpu[i]), shear[i], rotation[i]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image with background\n",
    "open_cv_image = np.array(add_background_img(foreground, background, max_foreground_size, scaler).convert('RGB'))\n",
    "\n",
    "# Convert RGB to BGR\n",
    "open_cv_image = open_cv_image[:, :, ::-1].copy() \n",
    "\n",
    "# augment image (rotate)\n",
    "# define rotation\n",
    "rotation = random.uniform(-85,85)\n",
    "shear = random.uniform(-10,10)\n",
    "# aug = iaa.Affine(rotate=45) # rotation\n",
    "\n",
    "# rotate_aug = ia.augmenters.geometric.Affine(rotate=rotation)\n",
    "# shear_aug = ia.augmenters.geometric.Affine(shear=shear)\n",
    "\n",
    "# exectue augmentation\n",
    "# new_img = rotate_aug.augment_image(open_cv_image)\n",
    "# new_img = shear_aug.augment_image(new_img)\n",
    "\n",
    "new_img = blur.augment_image(open_cv_image)\n",
    "\n",
    "# show\n",
    "img = cv2.cvtColor(new_img, cv2.COLOR_BGR2RGB)\n",
    "img = Image.fromarray(img)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
