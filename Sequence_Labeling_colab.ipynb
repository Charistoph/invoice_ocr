{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence Labeling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "BL7WMVGCvrV6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pandas_profiling\n",
        "import numpy as np\n",
        "import re\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uhnRJLTav_QO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://mikulskibartosz.name/how-to-load-data-from-google-drive-to-pandas-running-in-google-colaboratory-a7f6a033c997\n",
        "# nali.org/load-google-drive-csv-panda-dataframe-google-colab/\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9cXSdC0pwdyL",
        "colab_type": "code",
        "outputId": "35154442-b5d0-4de1-8970-650505d8c31e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1MI8PBiqNT-o6m0xYV8JYHUkqaqfTKkz2' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))\n",
        "  \n",
        "file_id = \"1727tj-xC0LJUZ77N2AdIyQWNJX7jKj1x\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: gen_invoices_labels_100k.csv, id: 1727tj-xC0LJUZ77N2AdIyQWNJX7jKj1x\n",
            "title: invoice_data, id: 1q7d9ckpy5o6bhRh0KKV0TJrzFMdf4pWN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "41VyW5acxkJK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copy data from Google Drive to Colaboratory\n",
        "download_path = os.path.expanduser('~/data')\n",
        "try:\n",
        "  os.makedirs(download_path)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "output_file = os.path.join(download_path, 'test.csv')\n",
        "\n",
        "temp_file = drive.CreateFile({'id': file_id})\n",
        "temp_file.GetContentFile(output_file)\n",
        "\n",
        "# open in pandas\n",
        "df = pd.read_csv(output_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9YL5T4R5vrWc",
        "colab_type": "code",
        "outputId": "9961d102-8b3a-45ef-f56b-413f032d937c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>price</th>\n",
              "      <th>str_input</th>\n",
              "      <th>str_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>29.11.2022</td>\n",
              "      <td>0.99</td>\n",
              "      <td>address : Perla Muniz 7000 Meadow Run Circle, ...</td>\n",
              "      <td>0000000000000000000000000000000000000000000000...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16.3.2028</td>\n",
              "      <td>0.21</td>\n",
              "      <td>Invoice\\nAdventure Direct Internet\\nDee Hacket...</td>\n",
              "      <td>0000000\\n0000000000000000000000000\\n0000000000...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28.4.2027</td>\n",
              "      <td>4.12</td>\n",
              "      <td>Invoice\\nPower Provider\\nJeremy Beers 1068 Sum...</td>\n",
              "      <td>0000000\\n00000000000000\\n000000000000000000000...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2026-23-7</td>\n",
              "      <td>4626.83</td>\n",
              "      <td>Electronic Interactive\\nJeromy Mcghee 8226 Pel...</td>\n",
              "      <td>0000000000000000000000\\n0000000000000000000000...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23.7.2020</td>\n",
              "      <td>0.37</td>\n",
              "      <td>address : Rolland Hutson 3023 Saint George Str...</td>\n",
              "      <td>0000000000000000000000000000000000000000000000...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         date    price                                          str_input  \\\n",
              "0  29.11.2022     0.99  address : Perla Muniz 7000 Meadow Run Circle, ...   \n",
              "1   16.3.2028     0.21  Invoice\\nAdventure Direct Internet\\nDee Hacket...   \n",
              "2   28.4.2027     4.12  Invoice\\nPower Provider\\nJeremy Beers 1068 Sum...   \n",
              "3   2026-23-7  4626.83  Electronic Interactive\\nJeromy Mcghee 8226 Pel...   \n",
              "4   23.7.2020     0.37  address : Rolland Hutson 3023 Saint George Str...   \n",
              "\n",
              "                                           str_label  \n",
              "0  0000000000000000000000000000000000000000000000...  \n",
              "1  0000000\\n0000000000000000000000000\\n0000000000...  \n",
              "2  0000000\\n00000000000000\\n000000000000000000000...  \n",
              "3  0000000000000000000000\\n0000000000000000000000...  \n",
              "4  0000000000000000000000000000000000000000000000...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "T0evVRtP5lNS",
        "colab_type": "code",
        "outputId": "df9b969d-39ae-4dd5-df4e-44fdf1f77391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hVfn0vsH531p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test\n",
        "\n",
        "with open('/content/gdrive/My Drive/Colab Notebooks/models/file_2.txt', 'w') as f:\n",
        "  f.write('content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "tWB6cKpXvrWu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# automatic EDA\n",
        "# https://nbviewer.jupyter.org/github/JosPolfliet/pandas-profiling/blob/master/examples/meteorites.ipynb\n",
        "# pandas_profiling.ProfileReport(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iFS_L5oUvrW3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inpu = []\n",
        "targ = []\n",
        "i = 0\n",
        "\n",
        "for i in range(len(df['str_input'])):\n",
        "    inp = df['str_input'][i].split(\"\\\\n\")\n",
        "    tar = df['str_label'][i].split(\"\\\\n\")\n",
        "\n",
        "    inp = [x for x in inp if len(x)>0]\n",
        "    tar = [x for x in tar if len(x)>0]\n",
        "\n",
        "    inpu.append(inp)\n",
        "    targ.append(tar)\n",
        "\n",
        "# for safety\n",
        "inpu = [x for x in inpu if len(x)>0]\n",
        "targ = [x for x in targ if len(x)>0]\n",
        "\n",
        "inpu = np.concatenate(inpu)\n",
        "targ = np.concatenate(targ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QTqMeTYMvrXA",
        "colab_type": "code",
        "outputId": "708d97aa-00ce-4ffb-dbc3-6043b5bb2342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "inpu[103], targ[103], inpu[-19], targ[-19]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('price EUR 0.24', '00000000001111', 'price EUR 16.15', '000000000011111')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "kNdXb42NvrXM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simple Bi-LSTM CRF"
      ]
    },
    {
      "metadata": {
        "id": "GJWpnhlLvrXQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\n",
        "\n",
        "# Viterbi algorithm - calculates maximum probability path for a hidden markov model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qFthqSLR0nGw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jEUeUg0avrXa",
        "colab_type": "code",
        "outputId": "f90414d9-0244-4de3-d0b7-4cbf03aa0e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4bc1c31f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "lYTlol3G1HiL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LQrxx78N01NR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, START_TAG, STOP_TAG):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.START_TAG = START_TAG\n",
        "        self.STOP_TAG = STOP_TAG\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[self.START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[self.STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[self.START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + \\\n",
        "            self.transitions[self.tag_to_ix[self.STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1)\n",
        "        tags = torch.cat(\n",
        "            [torch.tensor([self.tag_to_ix[self.START_TAG]], dtype=torch.long), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + \\\n",
        "            self.transitions[self.tag_to_ix[self.STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_ix[self.START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + \\\n",
        "            self.transitions[self.tag_to_ix[self.STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[self.START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pEInsAbovrX7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "EMBEDDING_DIM = 5\n",
        "HIDDEN_DIM = 4\n",
        "\n",
        "char_to_ix = {}\n",
        "for sentence in inpu:\n",
        "    for char in sentence:\n",
        "        if char not in char_to_ix:\n",
        "            char_to_ix[char] = len(char_to_ix)\n",
        "\n",
        "tag_to_ix = {}\n",
        "for sentence in targ:\n",
        "    for char in sentence:\n",
        "        if char not in tag_to_ix:\n",
        "            tag_to_ix[char] = len(tag_to_ix)\n",
        "\n",
        "tag_to_ix[START_TAG] = len(tag_to_ix)\n",
        "tag_to_ix[STOP_TAG] = len(tag_to_ix)\n",
        "\n",
        "all_data = np.transpose(np.stack([np.array(inpu), np.array(targ)]))\n",
        "training_data, test_data = train_test_split(all_data, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihDaCT2ovrYD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = BiLSTM_CRF(len(char_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, START_TAG, STOP_TAG)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SFAAav42vrYP",
        "colab_type": "code",
        "outputId": "2ea19b9f-1fcb-4885-873f-413208164c59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Check predictions before training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], char_to_ix)\n",
        "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
        "    print(model(precheck_sent))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor(45.1594), [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ousFDVikvrYc",
        "colab_type": "code",
        "outputId": "cecd66c9-e1b7-423c-ea77-46e512a2118d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17436
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
        "i = 0\n",
        "for epoch in range(1):\n",
        "    print(\"epoch: %i\" % epoch)\n",
        "    for sentence, tags in training_data[:100000]:\n",
        "        if(i % 10000 == 0):\n",
        "          print(\"iteration: %i\" % i)\n",
        "        if(i == 1000):\n",
        "          print(\"total estimated runtime %f hours\" % ((time.time() - start_time)*len(training_data)/(1000*60*60)))\n",
        "        i += 1\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, char_to_ix)\n",
        "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "        \n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0\n",
            "iteration: 0\n",
            "iteration: 100\n",
            "iteration: 200\n",
            "iteration: 300\n",
            "iteration: 400\n",
            "iteration: 500\n",
            "iteration: 600\n",
            "iteration: 700\n",
            "iteration: 800\n",
            "iteration: 900\n",
            "iteration: 1000\n",
            "total estimated runtime 14.036476 hours\n",
            "iteration: 1100\n",
            "iteration: 1200\n",
            "iteration: 1300\n",
            "iteration: 1400\n",
            "iteration: 1500\n",
            "iteration: 1600\n",
            "iteration: 1700\n",
            "iteration: 1800\n",
            "iteration: 1900\n",
            "iteration: 2000\n",
            "iteration: 2100\n",
            "iteration: 2200\n",
            "iteration: 2300\n",
            "iteration: 2400\n",
            "iteration: 2500\n",
            "iteration: 2600\n",
            "iteration: 2700\n",
            "iteration: 2800\n",
            "iteration: 2900\n",
            "iteration: 3000\n",
            "iteration: 3100\n",
            "iteration: 3200\n",
            "iteration: 3300\n",
            "iteration: 3400\n",
            "iteration: 3500\n",
            "iteration: 3600\n",
            "iteration: 3700\n",
            "iteration: 3800\n",
            "iteration: 3900\n",
            "iteration: 4000\n",
            "iteration: 4100\n",
            "iteration: 4200\n",
            "iteration: 4300\n",
            "iteration: 4400\n",
            "iteration: 4500\n",
            "iteration: 4600\n",
            "iteration: 4700\n",
            "iteration: 4800\n",
            "iteration: 4900\n",
            "iteration: 5000\n",
            "iteration: 5100\n",
            "iteration: 5200\n",
            "iteration: 5300\n",
            "iteration: 5400\n",
            "iteration: 5500\n",
            "iteration: 5600\n",
            "iteration: 5700\n",
            "iteration: 5800\n",
            "iteration: 5900\n",
            "iteration: 6000\n",
            "iteration: 6100\n",
            "iteration: 6200\n",
            "iteration: 6300\n",
            "iteration: 6400\n",
            "iteration: 6500\n",
            "iteration: 6600\n",
            "iteration: 6700\n",
            "iteration: 6800\n",
            "iteration: 6900\n",
            "iteration: 7000\n",
            "iteration: 7100\n",
            "iteration: 7200\n",
            "iteration: 7300\n",
            "iteration: 7400\n",
            "iteration: 7500\n",
            "iteration: 7600\n",
            "iteration: 7700\n",
            "iteration: 7800\n",
            "iteration: 7900\n",
            "iteration: 8000\n",
            "iteration: 8100\n",
            "iteration: 8200\n",
            "iteration: 8300\n",
            "iteration: 8400\n",
            "iteration: 8500\n",
            "iteration: 8600\n",
            "iteration: 8700\n",
            "iteration: 8800\n",
            "iteration: 8900\n",
            "iteration: 9000\n",
            "iteration: 9100\n",
            "iteration: 9200\n",
            "iteration: 9300\n",
            "iteration: 9400\n",
            "iteration: 9500\n",
            "iteration: 9600\n",
            "iteration: 9700\n",
            "iteration: 9800\n",
            "iteration: 9900\n",
            "iteration: 10000\n",
            "iteration: 10100\n",
            "iteration: 10200\n",
            "iteration: 10300\n",
            "iteration: 10400\n",
            "iteration: 10500\n",
            "iteration: 10600\n",
            "iteration: 10700\n",
            "iteration: 10800\n",
            "iteration: 10900\n",
            "iteration: 11000\n",
            "iteration: 11100\n",
            "iteration: 11200\n",
            "iteration: 11300\n",
            "iteration: 11400\n",
            "iteration: 11500\n",
            "iteration: 11600\n",
            "iteration: 11700\n",
            "iteration: 11800\n",
            "iteration: 11900\n",
            "iteration: 12000\n",
            "iteration: 12100\n",
            "iteration: 12200\n",
            "iteration: 12300\n",
            "iteration: 12400\n",
            "iteration: 12500\n",
            "iteration: 12600\n",
            "iteration: 12700\n",
            "iteration: 12800\n",
            "iteration: 12900\n",
            "iteration: 13000\n",
            "iteration: 13100\n",
            "iteration: 13200\n",
            "iteration: 13300\n",
            "iteration: 13400\n",
            "iteration: 13500\n",
            "iteration: 13600\n",
            "iteration: 13700\n",
            "iteration: 13800\n",
            "iteration: 13900\n",
            "iteration: 14000\n",
            "iteration: 14100\n",
            "iteration: 14200\n",
            "iteration: 14300\n",
            "iteration: 14400\n",
            "iteration: 14500\n",
            "iteration: 14600\n",
            "iteration: 14700\n",
            "iteration: 14800\n",
            "iteration: 14900\n",
            "iteration: 15000\n",
            "iteration: 15100\n",
            "iteration: 15200\n",
            "iteration: 15300\n",
            "iteration: 15400\n",
            "iteration: 15500\n",
            "iteration: 15600\n",
            "iteration: 15700\n",
            "iteration: 15800\n",
            "iteration: 15900\n",
            "iteration: 16000\n",
            "iteration: 16100\n",
            "iteration: 16200\n",
            "iteration: 16300\n",
            "iteration: 16400\n",
            "iteration: 16500\n",
            "iteration: 16600\n",
            "iteration: 16700\n",
            "iteration: 16800\n",
            "iteration: 16900\n",
            "iteration: 17000\n",
            "iteration: 17100\n",
            "iteration: 17200\n",
            "iteration: 17300\n",
            "iteration: 17400\n",
            "iteration: 17500\n",
            "iteration: 17600\n",
            "iteration: 17700\n",
            "iteration: 17800\n",
            "iteration: 17900\n",
            "iteration: 18000\n",
            "iteration: 18100\n",
            "iteration: 18200\n",
            "iteration: 18300\n",
            "iteration: 18400\n",
            "iteration: 18500\n",
            "iteration: 18600\n",
            "iteration: 18700\n",
            "iteration: 18800\n",
            "iteration: 18900\n",
            "iteration: 19000\n",
            "iteration: 19100\n",
            "iteration: 19200\n",
            "iteration: 19300\n",
            "iteration: 19400\n",
            "iteration: 19500\n",
            "iteration: 19600\n",
            "iteration: 19700\n",
            "iteration: 19800\n",
            "iteration: 19900\n",
            "iteration: 20000\n",
            "iteration: 20100\n",
            "iteration: 20200\n",
            "iteration: 20300\n",
            "iteration: 20400\n",
            "iteration: 20500\n",
            "iteration: 20600\n",
            "iteration: 20700\n",
            "iteration: 20800\n",
            "iteration: 20900\n",
            "iteration: 21000\n",
            "iteration: 21100\n",
            "iteration: 21200\n",
            "iteration: 21300\n",
            "iteration: 21400\n",
            "iteration: 21500\n",
            "iteration: 21600\n",
            "iteration: 21700\n",
            "iteration: 21800\n",
            "iteration: 21900\n",
            "iteration: 22000\n",
            "iteration: 22100\n",
            "iteration: 22200\n",
            "iteration: 22300\n",
            "iteration: 22400\n",
            "iteration: 22500\n",
            "iteration: 22600\n",
            "iteration: 22700\n",
            "iteration: 22800\n",
            "iteration: 22900\n",
            "iteration: 23000\n",
            "iteration: 23100\n",
            "iteration: 23200\n",
            "iteration: 23300\n",
            "iteration: 23400\n",
            "iteration: 23500\n",
            "iteration: 23600\n",
            "iteration: 23700\n",
            "iteration: 23800\n",
            "iteration: 23900\n",
            "iteration: 24000\n",
            "iteration: 24100\n",
            "iteration: 24200\n",
            "iteration: 24300\n",
            "iteration: 24400\n",
            "iteration: 24500\n",
            "iteration: 24600\n",
            "iteration: 24700\n",
            "iteration: 24800\n",
            "iteration: 24900\n",
            "iteration: 25000\n",
            "iteration: 25100\n",
            "iteration: 25200\n",
            "iteration: 25300\n",
            "iteration: 25400\n",
            "iteration: 25500\n",
            "iteration: 25600\n",
            "iteration: 25700\n",
            "iteration: 25800\n",
            "iteration: 25900\n",
            "iteration: 26000\n",
            "iteration: 26100\n",
            "iteration: 26200\n",
            "iteration: 26300\n",
            "iteration: 26400\n",
            "iteration: 26500\n",
            "iteration: 26600\n",
            "iteration: 26700\n",
            "iteration: 26800\n",
            "iteration: 26900\n",
            "iteration: 27000\n",
            "iteration: 27100\n",
            "iteration: 27200\n",
            "iteration: 27300\n",
            "iteration: 27400\n",
            "iteration: 27500\n",
            "iteration: 27600\n",
            "iteration: 27700\n",
            "iteration: 27800\n",
            "iteration: 27900\n",
            "iteration: 28000\n",
            "iteration: 28100\n",
            "iteration: 28200\n",
            "iteration: 28300\n",
            "iteration: 28400\n",
            "iteration: 28500\n",
            "iteration: 28600\n",
            "iteration: 28700\n",
            "iteration: 28800\n",
            "iteration: 28900\n",
            "iteration: 29000\n",
            "iteration: 29100\n",
            "iteration: 29200\n",
            "iteration: 29300\n",
            "iteration: 29400\n",
            "iteration: 29500\n",
            "iteration: 29600\n",
            "iteration: 29700\n",
            "iteration: 29800\n",
            "iteration: 29900\n",
            "iteration: 30000\n",
            "iteration: 30100\n",
            "iteration: 30200\n",
            "iteration: 30300\n",
            "iteration: 30400\n",
            "iteration: 30500\n",
            "iteration: 30600\n",
            "iteration: 30700\n",
            "iteration: 30800\n",
            "iteration: 30900\n",
            "iteration: 31000\n",
            "iteration: 31100\n",
            "iteration: 31200\n",
            "iteration: 31300\n",
            "iteration: 31400\n",
            "iteration: 31500\n",
            "iteration: 31600\n",
            "iteration: 31700\n",
            "iteration: 31800\n",
            "iteration: 31900\n",
            "iteration: 32000\n",
            "iteration: 32100\n",
            "iteration: 32200\n",
            "iteration: 32300\n",
            "iteration: 32400\n",
            "iteration: 32500\n",
            "iteration: 32600\n",
            "iteration: 32700\n",
            "iteration: 32800\n",
            "iteration: 32900\n",
            "iteration: 33000\n",
            "iteration: 33100\n",
            "iteration: 33200\n",
            "iteration: 33300\n",
            "iteration: 33400\n",
            "iteration: 33500\n",
            "iteration: 33600\n",
            "iteration: 33700\n",
            "iteration: 33800\n",
            "iteration: 33900\n",
            "iteration: 34000\n",
            "iteration: 34100\n",
            "iteration: 34200\n",
            "iteration: 34300\n",
            "iteration: 34400\n",
            "iteration: 34500\n",
            "iteration: 34600\n",
            "iteration: 34700\n",
            "iteration: 34800\n",
            "iteration: 34900\n",
            "iteration: 35000\n",
            "iteration: 35100\n",
            "iteration: 35200\n",
            "iteration: 35300\n",
            "iteration: 35400\n",
            "iteration: 35500\n",
            "iteration: 35600\n",
            "iteration: 35700\n",
            "iteration: 35800\n",
            "iteration: 35900\n",
            "iteration: 36000\n",
            "iteration: 36100\n",
            "iteration: 36200\n",
            "iteration: 36300\n",
            "iteration: 36400\n",
            "iteration: 36500\n",
            "iteration: 36600\n",
            "iteration: 36700\n",
            "iteration: 36800\n",
            "iteration: 36900\n",
            "iteration: 37000\n",
            "iteration: 37100\n",
            "iteration: 37200\n",
            "iteration: 37300\n",
            "iteration: 37400\n",
            "iteration: 37500\n",
            "iteration: 37600\n",
            "iteration: 37700\n",
            "iteration: 37800\n",
            "iteration: 37900\n",
            "iteration: 38000\n",
            "iteration: 38100\n",
            "iteration: 38200\n",
            "iteration: 38300\n",
            "iteration: 38400\n",
            "iteration: 38500\n",
            "iteration: 38600\n",
            "iteration: 38700\n",
            "iteration: 38800\n",
            "iteration: 38900\n",
            "iteration: 39000\n",
            "iteration: 39100\n",
            "iteration: 39200\n",
            "iteration: 39300\n",
            "iteration: 39400\n",
            "iteration: 39500\n",
            "iteration: 39600\n",
            "iteration: 39700\n",
            "iteration: 39800\n",
            "iteration: 39900\n",
            "iteration: 40000\n",
            "iteration: 40100\n",
            "iteration: 40200\n",
            "iteration: 40300\n",
            "iteration: 40400\n",
            "iteration: 40500\n",
            "iteration: 40600\n",
            "iteration: 40700\n",
            "iteration: 40800\n",
            "iteration: 40900\n",
            "iteration: 41000\n",
            "iteration: 41100\n",
            "iteration: 41200\n",
            "iteration: 41300\n",
            "iteration: 41400\n",
            "iteration: 41500\n",
            "iteration: 41600\n",
            "iteration: 41700\n",
            "iteration: 41800\n",
            "iteration: 41900\n",
            "iteration: 42000\n",
            "iteration: 42100\n",
            "iteration: 42200\n",
            "iteration: 42300\n",
            "iteration: 42400\n",
            "iteration: 42500\n",
            "iteration: 42600\n",
            "iteration: 42700\n",
            "iteration: 42800\n",
            "iteration: 42900\n",
            "iteration: 43000\n",
            "iteration: 43100\n",
            "iteration: 43200\n",
            "iteration: 43300\n",
            "iteration: 43400\n",
            "iteration: 43500\n",
            "iteration: 43600\n",
            "iteration: 43700\n",
            "iteration: 43800\n",
            "iteration: 43900\n",
            "iteration: 44000\n",
            "iteration: 44100\n",
            "iteration: 44200\n",
            "iteration: 44300\n",
            "iteration: 44400\n",
            "iteration: 44500\n",
            "iteration: 44600\n",
            "iteration: 44700\n",
            "iteration: 44800\n",
            "iteration: 44900\n",
            "iteration: 45000\n",
            "iteration: 45100\n",
            "iteration: 45200\n",
            "iteration: 45300\n",
            "iteration: 45400\n",
            "iteration: 45500\n",
            "iteration: 45600\n",
            "iteration: 45700\n",
            "iteration: 45800\n",
            "iteration: 45900\n",
            "iteration: 46000\n",
            "iteration: 46100\n",
            "iteration: 46200\n",
            "iteration: 46300\n",
            "iteration: 46400\n",
            "iteration: 46500\n",
            "iteration: 46600\n",
            "iteration: 46700\n",
            "iteration: 46800\n",
            "iteration: 46900\n",
            "iteration: 47000\n",
            "iteration: 47100\n",
            "iteration: 47200\n",
            "iteration: 47300\n",
            "iteration: 47400\n",
            "iteration: 47500\n",
            "iteration: 47600\n",
            "iteration: 47700\n",
            "iteration: 47800\n",
            "iteration: 47900\n",
            "iteration: 48000\n",
            "iteration: 48100\n",
            "iteration: 48200\n",
            "iteration: 48300\n",
            "iteration: 48400\n",
            "iteration: 48500\n",
            "iteration: 48600\n",
            "iteration: 48700\n",
            "iteration: 48800\n",
            "iteration: 48900\n",
            "iteration: 49000\n",
            "iteration: 49100\n",
            "iteration: 49200\n",
            "iteration: 49300\n",
            "iteration: 49400\n",
            "iteration: 49500\n",
            "iteration: 49600\n",
            "iteration: 49700\n",
            "iteration: 49800\n",
            "iteration: 49900\n",
            "iteration: 50000\n",
            "iteration: 50100\n",
            "iteration: 50200\n",
            "iteration: 50300\n",
            "iteration: 50400\n",
            "iteration: 50500\n",
            "iteration: 50600\n",
            "iteration: 50700\n",
            "iteration: 50800\n",
            "iteration: 50900\n",
            "iteration: 51000\n",
            "iteration: 51100\n",
            "iteration: 51200\n",
            "iteration: 51300\n",
            "iteration: 51400\n",
            "iteration: 51500\n",
            "iteration: 51600\n",
            "iteration: 51700\n",
            "iteration: 51800\n",
            "iteration: 51900\n",
            "iteration: 52000\n",
            "iteration: 52100\n",
            "iteration: 52200\n",
            "iteration: 52300\n",
            "iteration: 52400\n",
            "iteration: 52500\n",
            "iteration: 52600\n",
            "iteration: 52700\n",
            "iteration: 52800\n",
            "iteration: 52900\n",
            "iteration: 53000\n",
            "iteration: 53100\n",
            "iteration: 53200\n",
            "iteration: 53300\n",
            "iteration: 53400\n",
            "iteration: 53500\n",
            "iteration: 53600\n",
            "iteration: 53700\n",
            "iteration: 53800\n",
            "iteration: 53900\n",
            "iteration: 54000\n",
            "iteration: 54100\n",
            "iteration: 54200\n",
            "iteration: 54300\n",
            "iteration: 54400\n",
            "iteration: 54500\n",
            "iteration: 54600\n",
            "iteration: 54700\n",
            "iteration: 54800\n",
            "iteration: 54900\n",
            "iteration: 55000\n",
            "iteration: 55100\n",
            "iteration: 55200\n",
            "iteration: 55300\n",
            "iteration: 55400\n",
            "iteration: 55500\n",
            "iteration: 55600\n",
            "iteration: 55700\n",
            "iteration: 55800\n",
            "iteration: 55900\n",
            "iteration: 56000\n",
            "iteration: 56100\n",
            "iteration: 56200\n",
            "iteration: 56300\n",
            "iteration: 56400\n",
            "iteration: 56500\n",
            "iteration: 56600\n",
            "iteration: 56700\n",
            "iteration: 56800\n",
            "iteration: 56900\n",
            "iteration: 57000\n",
            "iteration: 57100\n",
            "iteration: 57200\n",
            "iteration: 57300\n",
            "iteration: 57400\n",
            "iteration: 57500\n",
            "iteration: 57600\n",
            "iteration: 57700\n",
            "iteration: 57800\n",
            "iteration: 57900\n",
            "iteration: 58000\n",
            "iteration: 58100\n",
            "iteration: 58200\n",
            "iteration: 58300\n",
            "iteration: 58400\n",
            "iteration: 58500\n",
            "iteration: 58600\n",
            "iteration: 58700\n",
            "iteration: 58800\n",
            "iteration: 58900\n",
            "iteration: 59000\n",
            "iteration: 59100\n",
            "iteration: 59200\n",
            "iteration: 59300\n",
            "iteration: 59400\n",
            "iteration: 59500\n",
            "iteration: 59600\n",
            "iteration: 59700\n",
            "iteration: 59800\n",
            "iteration: 59900\n",
            "iteration: 60000\n",
            "iteration: 60100\n",
            "iteration: 60200\n",
            "iteration: 60300\n",
            "iteration: 60400\n",
            "iteration: 60500\n",
            "iteration: 60600\n",
            "iteration: 60700\n",
            "iteration: 60800\n",
            "iteration: 60900\n",
            "iteration: 61000\n",
            "iteration: 61100\n",
            "iteration: 61200\n",
            "iteration: 61300\n",
            "iteration: 61400\n",
            "iteration: 61500\n",
            "iteration: 61600\n",
            "iteration: 61700\n",
            "iteration: 61800\n",
            "iteration: 61900\n",
            "iteration: 62000\n",
            "iteration: 62100\n",
            "iteration: 62200\n",
            "iteration: 62300\n",
            "iteration: 62400\n",
            "iteration: 62500\n",
            "iteration: 62600\n",
            "iteration: 62700\n",
            "iteration: 62800\n",
            "iteration: 62900\n",
            "iteration: 63000\n",
            "iteration: 63100\n",
            "iteration: 63200\n",
            "iteration: 63300\n",
            "iteration: 63400\n",
            "iteration: 63500\n",
            "iteration: 63600\n",
            "iteration: 63700\n",
            "iteration: 63800\n",
            "iteration: 63900\n",
            "iteration: 64000\n",
            "iteration: 64100\n",
            "iteration: 64200\n",
            "iteration: 64300\n",
            "iteration: 64400\n",
            "iteration: 64500\n",
            "iteration: 64600\n",
            "iteration: 64700\n",
            "iteration: 64800\n",
            "iteration: 64900\n",
            "iteration: 65000\n",
            "iteration: 65100\n",
            "iteration: 65200\n",
            "iteration: 65300\n",
            "iteration: 65400\n",
            "iteration: 65500\n",
            "iteration: 65600\n",
            "iteration: 65700\n",
            "iteration: 65800\n",
            "iteration: 65900\n",
            "iteration: 66000\n",
            "iteration: 66100\n",
            "iteration: 66200\n",
            "iteration: 66300\n",
            "iteration: 66400\n",
            "iteration: 66500\n",
            "iteration: 66600\n",
            "iteration: 66700\n",
            "iteration: 66800\n",
            "iteration: 66900\n",
            "iteration: 67000\n",
            "iteration: 67100\n",
            "iteration: 67200\n",
            "iteration: 67300\n",
            "iteration: 67400\n",
            "iteration: 67500\n",
            "iteration: 67600\n",
            "iteration: 67700\n",
            "iteration: 67800\n",
            "iteration: 67900\n",
            "iteration: 68000\n",
            "iteration: 68100\n",
            "iteration: 68200\n",
            "iteration: 68300\n",
            "iteration: 68400\n",
            "iteration: 68500\n",
            "iteration: 68600\n",
            "iteration: 68700\n",
            "iteration: 68800\n",
            "iteration: 68900\n",
            "iteration: 69000\n",
            "iteration: 69100\n",
            "iteration: 69200\n",
            "iteration: 69300\n",
            "iteration: 69400\n",
            "iteration: 69500\n",
            "iteration: 69600\n",
            "iteration: 69700\n",
            "iteration: 69800\n",
            "iteration: 69900\n",
            "iteration: 70000\n",
            "iteration: 70100\n",
            "iteration: 70200\n",
            "iteration: 70300\n",
            "iteration: 70400\n",
            "iteration: 70500\n",
            "iteration: 70600\n",
            "iteration: 70700\n",
            "iteration: 70800\n",
            "iteration: 70900\n",
            "iteration: 71000\n",
            "iteration: 71100\n",
            "iteration: 71200\n",
            "iteration: 71300\n",
            "iteration: 71400\n",
            "iteration: 71500\n",
            "iteration: 71600\n",
            "iteration: 71700\n",
            "iteration: 71800\n",
            "iteration: 71900\n",
            "iteration: 72000\n",
            "iteration: 72100\n",
            "iteration: 72200\n",
            "iteration: 72300\n",
            "iteration: 72400\n",
            "iteration: 72500\n",
            "iteration: 72600\n",
            "iteration: 72700\n",
            "iteration: 72800\n",
            "iteration: 72900\n",
            "iteration: 73000\n",
            "iteration: 73100\n",
            "iteration: 73200\n",
            "iteration: 73300\n",
            "iteration: 73400\n",
            "iteration: 73500\n",
            "iteration: 73600\n",
            "iteration: 73700\n",
            "iteration: 73800\n",
            "iteration: 73900\n",
            "iteration: 74000\n",
            "iteration: 74100\n",
            "iteration: 74200\n",
            "iteration: 74300\n",
            "iteration: 74400\n",
            "iteration: 74500\n",
            "iteration: 74600\n",
            "iteration: 74700\n",
            "iteration: 74800\n",
            "iteration: 74900\n",
            "iteration: 75000\n",
            "iteration: 75100\n",
            "iteration: 75200\n",
            "iteration: 75300\n",
            "iteration: 75400\n",
            "iteration: 75500\n",
            "iteration: 75600\n",
            "iteration: 75700\n",
            "iteration: 75800\n",
            "iteration: 75900\n",
            "iteration: 76000\n",
            "iteration: 76100\n",
            "iteration: 76200\n",
            "iteration: 76300\n",
            "iteration: 76400\n",
            "iteration: 76500\n",
            "iteration: 76600\n",
            "iteration: 76700\n",
            "iteration: 76800\n",
            "iteration: 76900\n",
            "iteration: 77000\n",
            "iteration: 77100\n",
            "iteration: 77200\n",
            "iteration: 77300\n",
            "iteration: 77400\n",
            "iteration: 77500\n",
            "iteration: 77600\n",
            "iteration: 77700\n",
            "iteration: 77800\n",
            "iteration: 77900\n",
            "iteration: 78000\n",
            "iteration: 78100\n",
            "iteration: 78200\n",
            "iteration: 78300\n",
            "iteration: 78400\n",
            "iteration: 78500\n",
            "iteration: 78600\n",
            "iteration: 78700\n",
            "iteration: 78800\n",
            "iteration: 78900\n",
            "iteration: 79000\n",
            "iteration: 79100\n",
            "iteration: 79200\n",
            "iteration: 79300\n",
            "iteration: 79400\n",
            "iteration: 79500\n",
            "iteration: 79600\n",
            "iteration: 79700\n",
            "iteration: 79800\n",
            "iteration: 79900\n",
            "iteration: 80000\n",
            "iteration: 80100\n",
            "iteration: 80200\n",
            "iteration: 80300\n",
            "iteration: 80400\n",
            "iteration: 80500\n",
            "iteration: 80600\n",
            "iteration: 80700\n",
            "iteration: 80800\n",
            "iteration: 80900\n",
            "iteration: 81000\n",
            "iteration: 81100\n",
            "iteration: 81200\n",
            "iteration: 81300\n",
            "iteration: 81400\n",
            "iteration: 81500\n",
            "iteration: 81600\n",
            "iteration: 81700\n",
            "iteration: 81800\n",
            "iteration: 81900\n",
            "iteration: 82000\n",
            "iteration: 82100\n",
            "iteration: 82200\n",
            "iteration: 82300\n",
            "iteration: 82400\n",
            "iteration: 82500\n",
            "iteration: 82600\n",
            "iteration: 82700\n",
            "iteration: 82800\n",
            "iteration: 82900\n",
            "iteration: 83000\n",
            "iteration: 83100\n",
            "iteration: 83200\n",
            "iteration: 83300\n",
            "iteration: 83400\n",
            "iteration: 83500\n",
            "iteration: 83600\n",
            "iteration: 83700\n",
            "iteration: 83800\n",
            "iteration: 83900\n",
            "iteration: 84000\n",
            "iteration: 84100\n",
            "iteration: 84200\n",
            "iteration: 84300\n",
            "iteration: 84400\n",
            "iteration: 84500\n",
            "iteration: 84600\n",
            "iteration: 84700\n",
            "iteration: 84800\n",
            "iteration: 84900\n",
            "iteration: 85000\n",
            "iteration: 85100\n",
            "iteration: 85200\n",
            "iteration: 85300\n",
            "iteration: 85400\n",
            "iteration: 85500\n",
            "iteration: 85600\n",
            "iteration: 85700\n",
            "iteration: 85800\n",
            "iteration: 85900\n",
            "iteration: 86000\n",
            "iteration: 86100\n",
            "iteration: 86200\n",
            "iteration: 86300\n",
            "iteration: 86400\n",
            "iteration: 86500\n",
            "iteration: 86600\n",
            "iteration: 86700\n",
            "iteration: 86800\n",
            "iteration: 86900\n",
            "iteration: 87000\n",
            "iteration: 87100\n",
            "iteration: 87200\n",
            "iteration: 87300\n",
            "iteration: 87400\n",
            "iteration: 87500\n",
            "iteration: 87600\n",
            "iteration: 87700\n",
            "iteration: 87800\n",
            "iteration: 87900\n",
            "iteration: 88000\n",
            "iteration: 88100\n",
            "iteration: 88200\n",
            "iteration: 88300\n",
            "iteration: 88400\n",
            "iteration: 88500\n",
            "iteration: 88600\n",
            "iteration: 88700\n",
            "iteration: 88800\n",
            "iteration: 88900\n",
            "iteration: 89000\n",
            "iteration: 89100\n",
            "iteration: 89200\n",
            "iteration: 89300\n",
            "iteration: 89400\n",
            "iteration: 89500\n",
            "iteration: 89600\n",
            "iteration: 89700\n",
            "iteration: 89800\n",
            "iteration: 89900\n",
            "iteration: 90000\n",
            "iteration: 90100\n",
            "iteration: 90200\n",
            "iteration: 90300\n",
            "iteration: 90400\n",
            "iteration: 90500\n",
            "iteration: 90600\n",
            "iteration: 90700\n",
            "iteration: 90800\n",
            "iteration: 90900\n",
            "iteration: 91000\n",
            "iteration: 91100\n",
            "iteration: 91200\n",
            "iteration: 91300\n",
            "iteration: 91400\n",
            "iteration: 91500\n",
            "iteration: 91600\n",
            "iteration: 91700\n",
            "iteration: 91800\n",
            "iteration: 91900\n",
            "iteration: 92000\n",
            "iteration: 92100\n",
            "iteration: 92200\n",
            "iteration: 92300\n",
            "iteration: 92400\n",
            "iteration: 92500\n",
            "iteration: 92600\n",
            "iteration: 92700\n",
            "iteration: 92800\n",
            "iteration: 92900\n",
            "iteration: 93000\n",
            "iteration: 93100\n",
            "iteration: 93200\n",
            "iteration: 93300\n",
            "iteration: 93400\n",
            "iteration: 93500\n",
            "iteration: 93600\n",
            "iteration: 93700\n",
            "iteration: 93800\n",
            "iteration: 93900\n",
            "iteration: 94000\n",
            "iteration: 94100\n",
            "iteration: 94200\n",
            "iteration: 94300\n",
            "iteration: 94400\n",
            "iteration: 94500\n",
            "iteration: 94600\n",
            "iteration: 94700\n",
            "iteration: 94800\n",
            "iteration: 94900\n",
            "iteration: 95000\n",
            "iteration: 95100\n",
            "iteration: 95200\n",
            "iteration: 95300\n",
            "iteration: 95400\n",
            "iteration: 95500\n",
            "iteration: 95600\n",
            "iteration: 95700\n",
            "iteration: 95800\n",
            "iteration: 95900\n",
            "iteration: 96000\n",
            "iteration: 96100\n",
            "iteration: 96200\n",
            "iteration: 96300\n",
            "iteration: 96400\n",
            "iteration: 96500\n",
            "iteration: 96600\n",
            "iteration: 96700\n",
            "iteration: 96800\n",
            "iteration: 96900\n",
            "iteration: 97000\n",
            "iteration: 97100\n",
            "iteration: 97200\n",
            "iteration: 97300\n",
            "iteration: 97400\n",
            "iteration: 97500\n",
            "iteration: 97600\n",
            "iteration: 97700\n",
            "iteration: 97800\n",
            "iteration: 97900\n",
            "iteration: 98000\n",
            "iteration: 98100\n",
            "iteration: 98200\n",
            "iteration: 98300\n",
            "iteration: 98400\n",
            "iteration: 98500\n",
            "iteration: 98600\n",
            "iteration: 98700\n",
            "iteration: 98800\n",
            "iteration: 98900\n",
            "iteration: 99000\n",
            "iteration: 99100\n",
            "iteration: 99200\n",
            "iteration: 99300\n",
            "iteration: 99400\n",
            "iteration: 99500\n",
            "iteration: 99600\n",
            "iteration: 99700\n",
            "iteration: 99800\n",
            "iteration: 99900\n",
            "--- 3866.776248693466 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dvexlGcDvrYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a1ddf0f6-b032-4603-b097-f881e29a83ec"
      },
      "cell_type": "code",
      "source": [
        "# Check predictions after training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], char_to_ix)\n",
        "    print(training_data[0][0])\n",
        "    print(model(precheck_sent))\n",
        "# We got it!"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Customer ID. 507217796253\n",
            "(tensor(164.7033), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vfcgn9d-vrYt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ]
    },
    {
      "metadata": {
        "id": "t_tvk3HtvrYz",
        "colab_type": "code",
        "outputId": "88ba98ef-557c-40a5-9f69-1285a0e32fe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "model_folder = \"/content/gdrive/My Drive/Colab Notebooks/models\"\n",
        "\n",
        "# create data folder\n",
        "try:\n",
        "    os.mkdir(model_folder)\n",
        "except:\n",
        "    print(\"%s folder already created!\" % model_folder)\n",
        "\n",
        "torch.save(model, \"%s/seq_labeling_BiLSTM_CRF.pt\" % model_folder)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/models folder already created!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type BiLSTM_CRF. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "if6l_kNvvrZO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ]
    },
    {
      "metadata": {
        "id": "XbE8hVi_vrZX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from bi_lstm_crf import BiLSTM_CRF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-jlU5Z_vrZl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = torch.load(\"%s/seq_labeling_BiLSTM_CRF.pt\" % model_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Si1QreA0vrZu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "test_sentences = np.transpose(test_data)\n",
        "for sentence in test_sentences[0][-100:]:\n",
        "    sentence = prepare_sequence(sentence, char_to_ix)\n",
        "    preds.append(model.forward(sentence)[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "ljMj-Z8nvrZ7",
        "colab_type": "code",
        "outputId": "14a898e3-820d-444e-a2b1-b93947edf3da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3490
        }
      },
      "cell_type": "code",
      "source": [
        "# for i in range(len(test_sentences[0])):\n",
        "for i in range(100):\n",
        "    print(test_sentences[0][-i])\n",
        "    print(\"\".join([str(elem) for elem in preds[-i]]))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  EUR 51.0\n",
            "000000000000\n",
            "Y.Crain@luptatumzzrildignissim.us\n",
            "000000000000000000000000000000000\n",
            "Omega Max Galaxy\n",
            "0000000000000000\n",
            "outrigger holiday switch: 2.13\n",
            "000000000000000000000000000000\n",
            "2026-18-7\n",
            "111111111\n",
            "Utwisi et enim accumsan ut et lorem nulla duis aliquam feugait iusto eufeugiat.\n",
            "0000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
            "flower diploma wind: 71.35\n",
            "00000000000000000000000000\n",
            "M.Teague@facilisispraesent.com\n",
            "000000000000000000000000000000\n",
            "Elouise@eavulputate.org\n",
            "00000000000000000000000\n",
            "Sub t. € 44.72\n",
            "00000000000000\n",
            "http://universal_power_alpha.co\n",
            "0000000000000000000000000000000\n",
            "Invoice # 8532499605\n",
            "00000000000000000000\n",
            "Invoice no. : 627325701814\n",
            "00000000000000000000000000\n",
            "address : Josef Mclaughlin 902 Sunrise Drive, 45840 Graceville\n",
            "00000000000000000000000000000000000000000000000000000000000000\n",
            "address : Antionette Denney 5014 Oak Glen Lane, 70039 Washington\n",
            "0000000000000000000000000000000000000000000000000000000000000000\n",
            "Feugiat consequat nulla vulputate nostrud suscipit eufeugiat erat.\n",
            "000000000000000000000000000000000000000000000000000000000000000000\n",
            "Ross Forney 7045 Periwinkle Street, 50612 Bradshaw\n",
            "00000000000000000000000000000000000000000000000000\n",
            "http://www.atlantic_contract_electronics.cn\n",
            "0000000000000000000000000000000000000000000\n",
            "Veniamquis lobortis dignissim et lobortis nulla velit eu facilisis tationullamcorper ea eum.\n",
            "00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
            "C.Cash@iustofeugait.net\n",
            "00000000000000000000000\n",
            "Star General\n",
            "000000000000\n",
            "C.Mckenzie@dignissimet.us\n",
            "0000000000000000000000000\n",
            "Jarred Braswell 4571 Merchants Trail, 26035 Lithia\n",
            "00000000000000000000000000000000000000000000000000\n",
            "Design Data Power\n",
            "00000000000000000\n",
            "Sub Tot. USD 8263.31\n",
            "00000000000000000000\n",
            "Minim consequat dolor eufeugiat.\n",
            "00000000000000000000000000000000\n",
            "polyester tomato: 3.48\n",
            "0000000000000000000000\n",
            "Adipiscing vero duis luptatumzzril ex dolore minim etaccumsan nislut tationullamcorper lorem.\n",
            "000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
            "address : Jaime Maddox 5921 Legend Parkway, 24504 Berkshire\n",
            "00000000000000000000000000000000000000000000000000000000000\n",
            "shark: 4.23\n",
            "00000000000\n",
            "Sammie Isaacson 7162 Shady Creek Parkway, 92604 Rock Point\n",
            "0000000000000000000000000000000000000000000000000000000000\n",
            "Emmanuel.Randle@sitsuscipit.net\n",
            "0000000000000000000000000000000\n",
            "Ex feugait autem diam etaccumsan ex delenit consequat euismod dolore accumsan erat ex.\n",
            "00000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
            "address : Elton Joiner 6799 Oakville Drive, 30417 Dwight\n",
            "00000000000000000000000000000000000000000000000000000000\n",
            "2026-19-5\n",
            "111111111\n",
            "Invoice # : 9647523694\n",
            "0000000000000000000000\n",
            "calf: 5.13\n",
            "0000000000\n",
            "9.11.2027\n",
            "111111111\n",
            "Sub t. USD 99.84\n",
            "0000000000000000\n",
            "Customer  Id. : 2256156\n",
            "00000000000000000000000\n",
            "Invoice no. : 886495294\n",
            "00000000000000000000000\n",
            "price EUR 0.98\n",
            "00000000002222\n",
            "Duis ullamcorper nisl ipsum adipiscing elitsed.\n",
            "00000000000000000000000000000000000000000000000\n",
            "Volutpat eros ullamcorper erat nibh lobortis nulla odio.\n",
            "00000000000000000000000000000000000000000000000000000000\n",
            "Vero enim dolore aliquam.\n",
            "0000000000000000000000000\n",
            "Sub t. € 4.2\n",
            "000000000000\n",
            "address : Juli Rohr 5041 Drayton Cove, 71480 Brockway\n",
            "00000000000000000000000000000000000000000000000000000\n",
            "Wisi molestie lorem vel dignissim nostrud suscipit duis vel at odio eros in illum.\n",
            "0000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
            "Invoice\n",
            "0000000\n",
            "Eu iustoodio delenit iusto nislut ex te lorem dolore nonummy vulputate blandit accumsan exerci.\n",
            "00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
            "mask pantry sailor mass bassoon 0.75\n",
            "000000000000000000000000000000000000\n",
            "Invoice\n",
            "0000000\n",
            "Invoice Number : 649048\n",
            "00000000000000000000000\n",
            "Blandit at Utwisi in blandit feugiat.\n",
            "0000000000000000000000000000000000000\n",
            "  EUR 0.7\n",
            "000000000\n",
            " 813382734\n",
            "0000000000\n",
            "Invoice no. 4864770\n",
            "0000000000000000000\n",
            "Duis suscipit vero nibh veniam nonummy autem elitsed volutpat sit Utwisi dolor.\n",
            "0000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
            "Oscar Kemp 2490 Rosehaven Avenue, 93246 Loami\n",
            "000000000000000000000000000000000000000000000\n",
            "  € 66.1\n",
            "00000000\n",
            "Balance Due $ 3153.77\n",
            "000000000000002222222\n",
            "Delenitaugue blandit lobortis tation iusto nonummy iustoodio.\n",
            "0000000000000000000000000000000000000000000000000000000000000\n",
            "Blandit illum adipiscing dignissim praesent esse nulla at feugait vero iustoodio blandit tation nulla.\n",
            "000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
            "address : Lorie Hofmann 3962 Redwood Place Drive, 26209 Deer Creek\n",
            "000000000000000000000000000000000000000000000000000000000000000000\n",
            "price EUR 4.11\n",
            "00000000002222\n",
            "Autem nulla wisi commodo vel in.\n",
            "00000000000000000000000000000000\n",
            "Invoice Number : 808879334\n",
            "00000000000000000000000000\n",
            "cause lamb: 92.92\n",
            "00000000000000000\n",
            "Commodo in te dignissim nulla aliquam exerci suscipit ut vel dignissim.\n",
            "00000000000000000000000000000000000000000000000000000000000000000000000\n",
            "Aliquip vel ea lobortis nibh tation.\n",
            "000000000000000000000000000000000000\n",
            "  $ 0.7\n",
            "0000000\n",
            "2028-20-7\n",
            "111111111\n",
            "address : Lonnie Olivares 835 Golden Chance Avenue, 99338 Sioux Falls\n",
            "000000000000000000000000000000000000000000000000000000000000000000000\n",
            "Nostrud te facilisis aliquip.\n",
            "00000000000000000000000000000\n",
            "Hyman@dolorodio.edu\n",
            "0000000000000000000\n",
            "Invoice Number 646427086\n",
            "000000000000000000000000\n",
            "plain cemetery beech moustache 201.97\n",
            "0000000000000000000000000000000000000\n",
            "Autem quis veniamquis commodo in dolore enim.\n",
            "000000000000000000000000000000000000000000000\n",
            "27.3.2025\n",
            "111111111\n",
            "week banker sofa salt: 45.5\n",
            "000000000000000000000000000\n",
            "Invoice no. 5538646\n",
            "0000000000000000000\n",
            "grandmother sidewalk: 350.56\n",
            "0000000000000000000000000000\n",
            "Invoice\n",
            "0000000\n",
            "Customer ID. : 81846849\n",
            "00000000000000000000000\n",
            "Customer Number : 24986279781\n",
            "00000000000000000000000000000\n",
            "Oscar@delenitauguenulla.com\n",
            "000000000000000000000000000\n",
            "Duis eum dolore euismod aliquip ea volutpat te nibh vel nulla.\n",
            "00000000000000000000000000000000000000000000000000000000000000\n",
            "transmission 46.34\n",
            "000000000000000000\n",
            "price $ 814.13\n",
            "00000000222222\n",
            "Rafaela Creighton 5836 Eagle Branch Trail, 68627 Garrett\n",
            "00000000000000000000000000000000000000000000000000000000\n",
            "Lucius Barnhart 6533 Hunters Lane, 94503 Troy\n",
            "000000000000000000000000000000000000000000000\n",
            "https://solutions_interactive_net.org\n",
            "0000000000000000000000000000000000000\n",
            "Jenifer Gordon 6945 Woodgate Street, 86043 Baltimore\n",
            "0000000000000000000000000000000000000000000000000000\n",
            "Invoice\n",
            "0000000\n",
            "sled 0.06\n",
            "000000000\n",
            "2022-15-2\n",
            "111111111\n",
            "violin: 0.03\n",
            "000000000000\n",
            "http://www.net_systems.ch\n",
            "0000000000000000000000000\n",
            "Te exerci aliquam ullamcorper dolore tincidunt nostrud nostrud.\n",
            "000000000000000000000000000000000000000000000000000000000000000\n",
            "Customer  Id. 55374858\n",
            "0000000000000000000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TTtx-KMlvraP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}