{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Rotation Colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "AHW8j4VSjFnL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# General Idea:\n",
        "# /content/gdrive/My Drive/Colab Notebooks/\n",
        "\n",
        "# Plan\n",
        "# 1. Add Background to Image dependent on image size (choose from x different backgrounds)\n",
        "# 2. Add some more distortions\n",
        "# 3. Rotate and sheer image\n",
        "# 4. Rescale images to fixed size.\n",
        "# 5. Write as generator\n",
        "# 6. Write CNN which predicts 2 angles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8mm3lIJSjFnd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "032de229-3017-4df1-b524-2bcbe59df933"
      },
      "cell_type": "code",
      "source": [
        "!pip install pillow\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "!pip install --upgrade imgaug\n",
        "import imgaug as ia\n",
        "print(ia.__version__)\n",
        "from imgaug import augmenters as iaa"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (5.4.1)\n",
            "Requirement already up-to-date: imgaug in /usr/local/lib/python3.6/dist-packages (0.2.7)\n",
            "Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.6.4.post2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug) (5.4.1)\n",
            "Requirement already satisfied, skipping upgrade: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (0.13.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (2.7.5)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.2)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug) (40.6.3)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image>=0.11.0->imgaug) (4.3.0)\n",
            "0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pmt-MLRlsIvs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Get Data"
      ]
    },
    {
      "metadata": {
        "id": "lqFZ8sN0jYIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://mikulskibartosz.name/how-to-load-data-from-google-drive-to-pandas-running-in-google-colaboratory-a7f6a033c997\n",
        "# nali.org/load-google-drive-csv-panda-dataframe-google-colab/\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PeigG19IkuBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "db968ead-e50f-404e-841a-b2bdaf8b5c8f"
      },
      "cell_type": "code",
      "source": [
        "drive = GoogleDrive(gauth) # Create GoogleDrive instance with authenticated GoogleAuth instance\n",
        "file_list = drive.ListFile({'q': \"'1MI8PBiqNT-o6m0xYV8JYHUkqaqfTKkz2' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))\n",
        "\n",
        "  \n",
        "invoice_img_folder_id = \"1lf4Ln_P_bFI5ZqZL4Cfr5-uVFPHoUjj5\"\n",
        "background_img_folder_id = \"1Gr9uJWxANEapNSovT8cP--QUzWeTv5Pw\"\n",
        "\n",
        "input_data_folder = \"invoice_img_data\"\n",
        "background_img_folder = \"background_img\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: invoice_img_data, id: 1lf4Ln_P_bFI5ZqZL4Cfr5-uVFPHoUjj5\n",
            "title: background_img, id: 1Gr9uJWxANEapNSovT8cP--QUzWeTv5Pw\n",
            "title: gen_invoices_labels_100k.csv, id: 1727tj-xC0LJUZ77N2AdIyQWNJX7jKj1x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZoGGk4ZXpJjC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_folder_get_data(folder_id, folder_name):\n",
        "  # create data folder\n",
        "  try:\n",
        "      os.mkdir(folder_name)\n",
        "  except FileExistsError:\n",
        "      print(\"%s folder already created!\" % folder_name)\n",
        "\n",
        "  os.chdir(folder_name)\n",
        "  \n",
        "  file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(folder_id)}).GetList()\n",
        "  file_list = [x for x in file_list if \"label\" not in x['title']]#[:30]\n",
        "\n",
        "  for i, file1 in enumerate(sorted(file_list, key = lambda x: x['title'])):\n",
        "      print('Downloading {} from GDrive ({}/{})'.format(file1['title'], i, len(file_list)))\n",
        "      file1.GetContentFile(file1['title'])\n",
        "      \n",
        "  os.chdir(\"..\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5n6jp2vzoPTQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create_folder_get_data(invoice_img_folder_id, input_data_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kwBHTwvyoPWJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create_folder_get_data(background_img_folder_id, background_img_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rrpGTPdBrRFX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "outputId": "d18bc25d-b669-446e-ab86-09321e3fef08"
      },
      "cell_type": "code",
      "source": [
        "!ls invoice_img_data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "out_0.png    out_145.png  out_190.png  out_235.png  out_280.png  out_55.png\n",
            "out_100.png  out_146.png  out_191.png  out_236.png  out_281.png  out_56.png\n",
            "out_101.png  out_147.png  out_192.png  out_237.png  out_282.png  out_57.png\n",
            "out_102.png  out_148.png  out_193.png  out_238.png  out_283.png  out_58.png\n",
            "out_103.png  out_149.png  out_194.png  out_239.png  out_284.png  out_59.png\n",
            "out_104.png  out_14.png   out_195.png  out_23.png   out_285.png  out_5.png\n",
            "out_105.png  out_150.png  out_196.png  out_240.png  out_286.png  out_60.png\n",
            "out_106.png  out_151.png  out_197.png  out_241.png  out_287.png  out_61.png\n",
            "out_107.png  out_152.png  out_198.png  out_242.png  out_288.png  out_62.png\n",
            "out_108.png  out_153.png  out_199.png  out_243.png  out_289.png  out_63.png\n",
            "out_109.png  out_154.png  out_19.png   out_244.png  out_28.png\t out_64.png\n",
            "out_10.png   out_155.png  out_1.png    out_245.png  out_290.png  out_65.png\n",
            "out_110.png  out_156.png  out_200.png  out_246.png  out_291.png  out_66.png\n",
            "out_111.png  out_157.png  out_201.png  out_247.png  out_292.png  out_67.png\n",
            "out_112.png  out_158.png  out_202.png  out_248.png  out_293.png  out_68.png\n",
            "out_113.png  out_159.png  out_203.png  out_249.png  out_294.png  out_69.png\n",
            "out_114.png  out_15.png   out_204.png  out_24.png   out_295.png  out_6.png\n",
            "out_115.png  out_160.png  out_205.png  out_250.png  out_296.png  out_70.png\n",
            "out_116.png  out_161.png  out_206.png  out_251.png  out_297.png  out_71.png\n",
            "out_117.png  out_162.png  out_207.png  out_252.png  out_298.png  out_72.png\n",
            "out_118.png  out_163.png  out_208.png  out_253.png  out_299.png  out_73.png\n",
            "out_119.png  out_164.png  out_209.png  out_254.png  out_29.png\t out_74.png\n",
            "out_11.png   out_165.png  out_20.png   out_255.png  out_2.png\t out_75.png\n",
            "out_120.png  out_166.png  out_210.png  out_256.png  out_30.png\t out_76.png\n",
            "out_121.png  out_167.png  out_211.png  out_257.png  out_31.png\t out_77.png\n",
            "out_122.png  out_168.png  out_212.png  out_258.png  out_32.png\t out_78.png\n",
            "out_123.png  out_169.png  out_213.png  out_259.png  out_33.png\t out_79.png\n",
            "out_124.png  out_16.png   out_214.png  out_25.png   out_34.png\t out_7.png\n",
            "out_125.png  out_170.png  out_215.png  out_260.png  out_35.png\t out_80.png\n",
            "out_126.png  out_171.png  out_216.png  out_261.png  out_36.png\t out_81.png\n",
            "out_127.png  out_172.png  out_217.png  out_262.png  out_37.png\t out_82.png\n",
            "out_128.png  out_173.png  out_218.png  out_263.png  out_38.png\t out_83.png\n",
            "out_129.png  out_174.png  out_219.png  out_264.png  out_39.png\t out_84.png\n",
            "out_12.png   out_175.png  out_21.png   out_265.png  out_3.png\t out_85.png\n",
            "out_130.png  out_176.png  out_220.png  out_266.png  out_40.png\t out_86.png\n",
            "out_131.png  out_177.png  out_221.png  out_267.png  out_41.png\t out_87.png\n",
            "out_132.png  out_178.png  out_222.png  out_268.png  out_42.png\t out_88.png\n",
            "out_133.png  out_179.png  out_223.png  out_269.png  out_43.png\t out_89.png\n",
            "out_134.png  out_17.png   out_224.png  out_26.png   out_44.png\t out_8.png\n",
            "out_135.png  out_180.png  out_225.png  out_270.png  out_45.png\t out_90.png\n",
            "out_136.png  out_181.png  out_226.png  out_271.png  out_46.png\t out_91.png\n",
            "out_137.png  out_182.png  out_227.png  out_272.png  out_47.png\t out_92.png\n",
            "out_138.png  out_183.png  out_228.png  out_273.png  out_48.png\t out_93.png\n",
            "out_139.png  out_184.png  out_229.png  out_274.png  out_49.png\t out_94.png\n",
            "out_13.png   out_185.png  out_22.png   out_275.png  out_4.png\t out_95.png\n",
            "out_140.png  out_186.png  out_230.png  out_276.png  out_50.png\t out_96.png\n",
            "out_141.png  out_187.png  out_231.png  out_277.png  out_51.png\t out_97.png\n",
            "out_142.png  out_188.png  out_232.png  out_278.png  out_52.png\t out_98.png\n",
            "out_143.png  out_189.png  out_233.png  out_279.png  out_53.png\t out_99.png\n",
            "out_144.png  out_18.png   out_234.png  out_27.png   out_54.png\t out_9.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CuPcPnDWoPRA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "fbe530e4-20c5-4ae3-a809-494eaeb3581e"
      },
      "cell_type": "code",
      "source": [
        "!ls background_img"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1538846.jpg\n",
            "33971803-potted-grass-flower-over-wooden-table-background-with-copy-space.jpg\n",
            "43581859-succulent-in-pot-wooden-table-background-with-copy-space.jpg\n",
            "79876024-abstract-surface-white-wood-table-texture-background-close-up-of-dark-rustic-wall-made-of-white-wood.jpg\n",
            "81915338-abstract-rustic-surface-white-wood-table-texture-background-close-up-of-rustic-wall-made-of-white-wo.jpg\n",
            "93268346-white-background-wooden-table-surface-texture-planks-close-up.jpg\n",
            "abstract-light-grey-background-vector-13747865.jpg\n",
            "Dark-Grey-Background-Texture-10.jpg\n",
            "f50524ee5f161f437400aaf215c9e12f.jpg\n",
            "hazelnut-falling-on-a-wooden-table-on-a-black-background-slow-motion-close-up_rahktoix__F0000.png\n",
            "images1.jpg\n",
            "images2.jpg\n",
            "images.jpg\n",
            "index1.png\n",
            "index.jpg\n",
            "mobile-phone-with-blank-screen-on-wooden-table-background-top-view-with-copy-space_1253-984.jpg\n",
            "pexels-photo-242236.jpeg\n",
            "pouring-of-lentils-on-white-background-this-video-was-shoot-using-custom-light-set-up-with-additional-custom-build-underneath-light-system-to-eliminate-shadows_n17dyaafx__S0000.jpg\n",
            "table-background_wood_table_background_4_background_check_all.jpg\n",
            "Table_surface.jpg\n",
            "Table-Top-Background-1.png\n",
            "white-background-2.jpg\n",
            "wood-table-top-on-isolated-background-vector-7418707.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aEJWtlUujFnZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Augmentation"
      ]
    },
    {
      "metadata": {
        "id": "tXA43yLXjFnk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_foreground_size = 300\n",
        "scaler = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fkeSplL3jFnt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_background_img(foreground, background, max_foreground_size, scaler):\n",
        "    width = foreground.size[0]\n",
        "    height = foreground.size[1]\n",
        "\n",
        "    if(width > height):\n",
        "        percentage = max_foreground_size/width\n",
        "        max_size = int(width*scaler*percentage)\n",
        "    else:\n",
        "        percentage = max_foreground_size/height\n",
        "        max_size = int(height*scaler*percentage)\n",
        "        \n",
        "#     print(width, height, percentage)\n",
        "\n",
        "    foreground = foreground.resize((int(width*percentage), int(height*percentage)), Image.ANTIALIAS)\n",
        "\n",
        "    background = background.resize((max_size, max_size), Image.ANTIALIAS)\n",
        "\n",
        "    margin_w = int((background.size[0]-foreground.size[0])/2)\n",
        "    margin_h = int((background.size[1]-foreground.size[1])/2)\n",
        "\n",
        "    # foreground.show()\n",
        "    background.paste(foreground, (margin_w, margin_h))\n",
        "#     background.show()\n",
        "\n",
        "    return background"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZkuuL9_QjFnz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def augment_img(foreground, background, max_foreground_size, scaler):\n",
        "    # load image with background\n",
        "    open_cv_image = np.array(add_background_img(foreground, background, max_foreground_size, scaler).convert('RGB'))\n",
        "\n",
        "    # Convert RGB to BGR\n",
        "    open_cv_image = open_cv_image[:, :, ::-1].copy() \n",
        "\n",
        "    # define augmentations\n",
        "    rotation = random.randint(-85,85)\n",
        "    shear = random.randint(-10,10)\n",
        "\n",
        "    # 2. Add some more distortions\n",
        "    blur_aug = ia.augmenters.blur.MotionBlur(k=(3,10), angle=(0, 360), direction=(-1.0, 1.0))\n",
        "    \n",
        "    # 3. Rotate and sheer image\n",
        "    rotate_aug = ia.augmenters.geometric.Affine(rotate=rotation)\n",
        "    shear_aug = ia.augmenters.geometric.Affine(shear=shear)\n",
        "    \n",
        "    # exectue augmentation\n",
        "    new_img = blur_aug.augment_image(open_cv_image)\n",
        "    new_img = rotate_aug.augment_image(new_img)\n",
        "    new_img = shear_aug.augment_image(new_img)\n",
        "\n",
        "    # show\n",
        "    img = cv2.cvtColor(new_img, cv2.COLOR_BGR2RGB)\n",
        "    img = Image.fromarray(img)\n",
        "#     img.show()\n",
        "\n",
        "    # crop image\n",
        "    rand_scaler = scaler*random.uniform(0.7,1.3)\n",
        "#     print(\"rand_scaler\", rand_scaler)\n",
        "    crop_side_percentage = (rand_scaler-1)/(2*rand_scaler)\n",
        "\n",
        "    area = (\n",
        "        img.size[0]*crop_side_percentage, # width left\n",
        "        img.size[1]*crop_side_percentage, # height top\n",
        "        img.size[0]*(1-crop_side_percentage), # width right\n",
        "        img.size[1]*(1-crop_side_percentage), # height bottom\n",
        "    )\n",
        "    cropped_img = img.crop(area)\n",
        "#     cropped_img.show()\n",
        "    \n",
        "    return cropped_img, rotation, shear"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "EWwkK0SJjFn5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test augmentation\n",
        "\n",
        "# background = Image.open(\"background_img/Table_surface.jpg\")\n",
        "# foreground = Image.open(\"invoice_img_data/out_0.png\")\n",
        "\n",
        "# final_img, rotation, shear = augment_img(foreground, background, max_foreground_size, scaler)\n",
        "# final_img.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f6A6k5nyjFn_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iEMQoFlVjFoG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_img_names = [x for x in os.listdir(input_data_folder) if \"_label\" not in x and \".png\" in x]\n",
        "background_img_names = [\n",
        "    x for x in os.listdir(background_img_folder) if \"_label\" not in x and \".png\" in x or \".jpg\" in x or \".jpeg\" in x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ZRi820djFoR",
        "colab_type": "code",
        "outputId": "3a16c5f9-1014-457a-dddd-9f40bf1a4abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(input_img_names), len(background_img_names)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "2IUtnxP4jFoh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "u2DMFAzutqQt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !!pip uninstall torch\n",
        "!pip install -q torch==1.0.0 torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZsPco7OsjFoj",
        "colab_type": "code",
        "outputId": "fbc533d3-f62e-49c3-b6ac-fe7d20c84c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from numpy import argmax\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "from torch import nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "# https://stackoverflow.com/questions/42480111/model-summary-in-pytorch\n",
        "# from torchsummary import summary\n",
        "\n",
        "# https://github.com/lanpa/tensorboardX\n",
        "# from tensorboardX import SummaryWriter\n",
        "# writer = SummaryWriter(\"logs/image_rotation\")\n",
        "\n",
        "!pip install wandb\n",
        "import wandb"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.0\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.6/dist-packages (0.6.32)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.5.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.5.3)\n",
            "Requirement already satisfied: gql>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.11.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.18.4)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.1.11)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.7.5)\n",
            "Requirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.9.0)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.6.8)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: promise>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from gql>=0.1.0->wandb) (2.2.1)\n",
            "Requirement already satisfied: graphql-core>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from gql>=0.1.0->wandb) (2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.22)\n",
            "Requirement already satisfied: gitdb2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (2.0.5)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (3.13)\n",
            "Requirement already satisfied: argh>=0.24.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (0.26.2)\n",
            "Requirement already satisfied: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
            "Requirement already satisfied: rx>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from graphql-core>=0.5.0->gql>=0.1.0->wandb) (1.6.1)\n",
            "Requirement already satisfied: smmap2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from gitdb2>=2.0.0->GitPython>=1.0.0->wandb) (2.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e6BQQVKEjFoo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# device = torch.device('cpu')\n",
        "device = torch.device('cuda') # Uncomment this to run on GPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YRst3D05jFou",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class depthwise_separable_conv(nn.Module):\n",
        "    def __init__(self, nin, nout, ksize, padd):\n",
        "        super(depthwise_separable_conv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=ksize, padding=padd, groups=nin)\n",
        "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UxrQIeq3jFo0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class conv_max_step(nn.Module):\n",
        "    def __init__(self, nin, nout, ksize, padd):\n",
        "        super(conv_max_step, self).__init__()\n",
        "        self.conv = depthwise_separable_conv(nin, nout, ksize, padd)\n",
        "        self.batchn = nn.BatchNorm2d(nout)\n",
        "        self.tanh = nn.ReLU()\n",
        "        self.maxp = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.batchn(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.maxp(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "daWBdBBsjFo4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, max_img_size, nchannel, nclasses_1, nclasses_2):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        nin = nchannel\n",
        "        nout = int(nin*2)\n",
        "        self.cm1 = conv_max_step(nin, nout, 7, 3)\n",
        "        max_img_size = max_img_size/2\n",
        "\n",
        "        nin = nout\n",
        "        nout = int(nin*2)\n",
        "        self.cm2 = conv_max_step(nin, nout, 3, 1)\n",
        "        max_img_size = max_img_size/2\n",
        "\n",
        "        nin = nout\n",
        "        nout = int(nin*2)\n",
        "        self.cm3 = conv_max_step(nin, nout, 3, 1)\n",
        "        max_img_size = max_img_size/2\n",
        "\n",
        "        nin = nout\n",
        "        nout = int(nin*2)\n",
        "        self.cm4 = conv_max_step(nin, nout, 3, 1)\n",
        "        max_img_size = int(max_img_size/2)\n",
        "\n",
        "        self.lin_dim = nout*max_img_size*max_img_size\n",
        "        self.fc1 = nn.Linear(in_features=self.lin_dim, out_features=nclasses_1)\n",
        "        self.fc2 = nn.Linear(in_features=self.lin_dim, out_features=nclasses_2)\n",
        "        self.softm1 = nn.LogSoftmax(dim=1)\n",
        "        self.softm2 = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cm1(x)\n",
        "        x = self.cm2(x)\n",
        "        x = self.cm3(x)\n",
        "        x = self.cm4(x)\n",
        "        x = x.view(-1, self.lin_dim)\n",
        "        x1 = self.fc1(x)\n",
        "        x2 = self.fc2(x)\n",
        "        out1 = self.softm1(x1)\n",
        "        out2 = self.softm2(x2)\n",
        "       \n",
        "        return out1, out2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aOLconp5lyne",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encoder_func(encode, arr):\n",
        "  integer_encoded = [encode[x] for x in arr]\n",
        "#   print(integer_encoded)\n",
        "\n",
        "  # one hot encode\n",
        "  onehot_encoded = list()\n",
        "  for value in integer_encoded:\n",
        "    item = [0 for _ in range(len(encode))]\n",
        "    item[value] = 1\n",
        "    onehot_encoded.append(item)\n",
        "#     print(sum(item), item)\n",
        "\n",
        "  return onehot_encoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7RbSO0vfjFo9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def training_data_generator(input_img_names, background_img_names, itr, batch_size, rot_to_int, she_to_int, device):\n",
        "    inpu = []\n",
        "    rotation = []\n",
        "    shear = []\n",
        "\n",
        "    # put into generator function for evaluation\n",
        "    for img_name in input_img_names[itr*batch_size:(itr+1)*batch_size]:\n",
        "        background_name = random.choice(background_img_names)\n",
        "\n",
        "#         print(os.path.join(input_data_folder, img_name))\n",
        "        foreground = Image.open(os.path.join(input_data_folder, img_name))\n",
        "#         print(os.path.join(background_img_folder, background_name))\n",
        "        background = Image.open(os.path.join(background_img_folder, background_name))\n",
        "\n",
        "        curr_img, rot, she = augment_img(foreground, background, max_foreground_size, scaler)\n",
        "\n",
        "        curr_img = curr_img.resize((max_img_size, max_img_size), Image.ANTIALIAS)\n",
        "        curr_img = np.array(curr_img)\n",
        "\n",
        "        inpu.append(curr_img)\n",
        "        rotation.append(rot)\n",
        "        shear.append(she)\n",
        "\n",
        "    # move channel to second index position\n",
        "    inpu = np.swapaxes(np.array(inpu), 3, -3)\n",
        "    X = torch.from_numpy(inpu).float().to(device)\n",
        "\n",
        "    rotation_enc = encoder_func(rot_to_int, rotation)\n",
        "    rotation_enc = np.array(rotation_enc)\n",
        "    y1 = torch.from_numpy(rotation_enc).float().to(device)\n",
        "    \n",
        "    shear_enc = encoder_func(she_to_int, shear)\n",
        "    shear_enc = np.array(shear_enc)\n",
        "    y2 = torch.from_numpy(shear_enc).float().to(device)\n",
        "    \n",
        "    return X, y1, y2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1H27QYBE4-NF",
        "colab_type": "code",
        "outputId": "c44699ac-1e71-4473-889e-f5fd70444d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# preperation for saving model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e4FPPWx6jFpB",
        "colab_type": "code",
        "outputId": "2f56b06b-cb98-461b-db42-90201f72e677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 3\n",
        "batch_size = 254\n",
        "learning_rate = 1e-5\n",
        "\n",
        "n_batches =  int(np.ceil(len(input_img_names)/batch_size))\n",
        "\n",
        "print(max_epochs, batch_size, n_batches)\n",
        "\n",
        "max_rotation = 85\n",
        "max_shear = 10\n",
        "\n",
        "rot_to_int = dict((c, i) for i, c in enumerate(range(-max_rotation,max_rotation+1)))\n",
        "she_to_int = dict((c, i) for i, c in enumerate(range(-max_shear,max_shear+1)))\n",
        "\n",
        "max_img_size = 64\n",
        "inp_channels = 3\n",
        "nclasses_1 = max_rotation*2+1\n",
        "nclasses_2 = max_shear*2+1"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 254 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mJKO57IN3JAq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test if GPU is being used\n",
        "cudnn.benchmark = True\n",
        "\n",
        "# model\n",
        "model = Model(max_img_size, inp_channels, nclasses_1, nclasses_2)\n",
        "model.to(device)\n",
        "# loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "wandb.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xL2f5pLB2S3f",
        "colab_type": "code",
        "outputId": "62abedf8-eacb-49e4-92ca-c26bec2294ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in tqdm(range(max_epochs)):\n",
        "#     print(\"epoch: %i\" % epoch)\n",
        "    for itr in range(n_batches):\n",
        "        # get training data from generator\n",
        "        X, y1, y2 = training_data_generator(input_img_names, background_img_names, itr, batch_size, rot_to_int, she_to_int, device)\n",
        "        print(X.shape, y1.shape, y2.shape)\n",
        "\n",
        "        # Forward pass: compute predicted y by passing x to the model.\n",
        "        y_pred1, y_pred2 = model.forward(X)\n",
        "\n",
        "        # Compute and print loss.\n",
        "        loss = criterion(y_pred1, torch.max(y1.long(), 1)[1]) + criterion(y_pred2, torch.max(y2.long(), 1)[1])\n",
        "\n",
        "        # Zero the gradients before running the backward pass.\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Backward pass: compute gradient of the loss.\n",
        "        loss.backward()\n",
        "\n",
        "        # Calling the step function on an Optimizer makes an update to its parameters\n",
        "        optimizer.step()\n",
        "    \n",
        "    wandb.log({'loss': loss.item()})\n",
        "    print(epoch, loss.item())\n",
        "#     writer.add_scalar(\"total_loss\", loss.item(), epoch)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([254, 3, 64, 64]) torch.Size([254, 171]) torch.Size([254, 21])\n",
            "x1 torch.Size([254, 171])\n",
            "x2 torch.Size([254, 21])\n",
            "out1 torch.Size([254, 171])\n",
            "out2 torch.Size([254, 21])\n",
            "torch.Size([46, 3, 64, 64]) torch.Size([46, 171]) torch.Size([46, 21])\n",
            "x1 torch.Size([46, 171])\n",
            "x2 torch.Size([46, 21])\n",
            "out1 torch.Size([46, 171])\n",
            "out2 torch.Size([46, 21])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-62b16ef9a494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#     writer.add_scalar(\"total_loss\", loss.item(), epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(row, commit, *args, **kargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         raise ValueError(\n\u001b[0;32m--> 444\u001b[0;31m             \"You must call `wandb.init` in the same process before calling log\")\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You must call `wandb.init` in the same process before calling log"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "P4Cn0-CC4yfw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ]
    },
    {
      "metadata": {
        "id": "ekSDtlFo4wze",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_folder = \"/content/gdrive/My Drive/Colab Notebooks/models\"\n",
        "\n",
        "# create data folder\n",
        "try:\n",
        "    os.mkdir(model_folder)\n",
        "except:\n",
        "    print(\"%s folder already created!\" % model_folder)\n",
        "\n",
        "torch.save(model, \"%s/image_rotation.pt\" % model_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XEx4cl8VjFpL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sudo Inference"
      ]
    },
    {
      "metadata": {
        "id": "WN_nJtd2jFpP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def correct_image(final_img, shear, rotation):\n",
        "    shear_aug_ = ia.augmenters.geometric.Affine(shear=-shear)\n",
        "    rotate_aug_ = ia.augmenters.geometric.Affine(rotate=-rotation)\n",
        "\n",
        "    reverse_img = np.array(final_img.convert('RGB'))\n",
        "\n",
        "    # Convert RGB to BGR\n",
        "    reverse_img = reverse_img[:, :, ::-1].copy()\n",
        "\n",
        "    # reverse augmentation\n",
        "    reverse_img = shear_aug_.augment_image(reverse_img)\n",
        "    reverse_img = rotate_aug_.augment_image(reverse_img)\n",
        "\n",
        "    # show\n",
        "    img = cv2.cvtColor(reverse_img, cv2.COLOR_BGR2RGB)\n",
        "    img = Image.fromarray(img)\n",
        "    \n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yB8CF9kkjFpV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "correct_image(final_img).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N6RNyQG4jFpe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(batch_size):\n",
        "    Image.fromarray(inpu[i]).show()\n",
        "    correct_image(Image.fromarray(inpu[i]), shear[i], rotation[i]).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pyL3V1ZPjFpm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load image with background\n",
        "open_cv_image = np.array(add_background_img(foreground, background, max_foreground_size, scaler).convert('RGB'))\n",
        "\n",
        "# Convert RGB to BGR\n",
        "open_cv_image = open_cv_image[:, :, ::-1].copy() \n",
        "\n",
        "# augment image (rotate)\n",
        "# define rotation\n",
        "rotation = random.uniform(-85,85)\n",
        "shear = random.uniform(-10,10)\n",
        "# aug = iaa.Affine(rotate=45) # rotation\n",
        "\n",
        "# rotate_aug = ia.augmenters.geometric.Affine(rotate=rotation)\n",
        "# shear_aug = ia.augmenters.geometric.Affine(shear=shear)\n",
        "\n",
        "# exectue augmentation\n",
        "# new_img = rotate_aug.augment_image(open_cv_image)\n",
        "# new_img = shear_aug.augment_image(new_img)\n",
        "\n",
        "new_img = blur.augment_image(open_cv_image)\n",
        "\n",
        "# show\n",
        "img = cv2.cvtColor(new_img, cv2.COLOR_BGR2RGB)\n",
        "img = Image.fromarray(img)\n",
        "img.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "REH3PwRnjFpu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uXvaJjrqjFp3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ta370V9FjFp7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}